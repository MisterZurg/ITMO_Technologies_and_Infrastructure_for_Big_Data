{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Lab\n",
    "\n",
    "## General info\n",
    "\n",
    "Using `pyspark` you'll have to complete 6 tasks (the tasks are listed below). This lab is **REQUIRED**.\n",
    "\n",
    "This lab also will be checked automatically. Thus, please be careful and **read task conditions attentively**.\n",
    "\n",
    "\n",
    "<font size=\"5\" style=\"background-color:orange;\"><b>PLAGIARISM IS PROHIBITED! <br> PUNISHMENT UP TO NON-ADMISSION TO THE EXAM!</b></font>\n",
    "\n",
    "---\n",
    "\n",
    "The lab deadline is ***19.05 23:59***. To submit the lab, please put the completed notebook in your **NFS home directory** in the following path:\n",
    "\n",
    "(path in gateway.st) `/nfs/home/<your-login>/spark-lab.ipynb`\n",
    "\n",
    "(path in jupyter containers) `/home/jovyan/nfs-home/spark-lab.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "This notebook is available in your Jupyter servers by following path `/home/jovyan/shared-data/notebooks/BDML/SparkLab-Template.ipynb`.\n",
    "\n",
    "---\n",
    "\n",
    "## Lab description\n",
    "\n",
    "During the lab you'll have to analyze social network (VK) data. The data contain user posts, group posts, user likes.\n",
    "Detailed datasets info and tasks description is provided below.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Tasks list\n",
    "\n",
    "The detailed information about each task is provided below.\n",
    "\n",
    "- 1. Find top posts:\n",
    "    - 1a. by likes count (1 point)\n",
    "    - 1b. by comments count (1 point)\n",
    "    - 1c. by reposts count (1 point)\n",
    "- 2. Find top users:\n",
    "    - 2a. by likes are **received** (1 point)\n",
    "    - 2b. by reposts are **made** (1 point)\n",
    "- 3. Extract made users' reposts from the **ITMO group** (3 points)\n",
    "- 4. Extrace emojis in user posts and find top positive emojis, top neutral emojis and top negative emojis. (3 points)\n",
    "- 5. Probable fans (4 points)\n",
    "- 6. Probable friends (5 points)\n",
    "\n",
    "---\n",
    "\n",
    "## How to complete a task\n",
    "\n",
    "For each task you will have a predefined function signature. ***DO NOT MODIFY SIGNATURES!***\n",
    "\n",
    "You'll have to put the code to transform Spark DataFrame and return modified dataframe from the function.\n",
    "\n",
    "Limitations:\n",
    "- Imports inside the functions are not allowed (except task 4).\n",
    "- Access to global variables are not allowed. If you'll use global variable inside your functions, Lab-checker will not provide the variable to the execution context and the task will fail with `NameError` exception.\n",
    "- Dynamically generated code execution is not allowed (`eval`, `exec`, `compile`, etc)\n",
    "- Inner functions and closures are not allowed (except task 4)\n",
    "\n",
    "---\n",
    "\n",
    "## FAQ\n",
    "\n",
    "FAQ section is located in the end of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import types as T, functions as F, SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import StorageLevel\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOGIN = \"YOUR_LOGIN\"  # Your gateway.st login\n",
    "APP_NAME = \"PySparkLab_APP\"  # Any name for your Spark-app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZED_APP_NAME = APP_NAME.replace('/', '_').replace(':', '_').replace(' ', '_').replace('\\\\', '_')\n",
    "\n",
    "APPS_TMP_DIR = os.path.join(os.getcwd(), \"tmp\")\n",
    "APPS_CONF_DIR = os.path.join(os.getcwd(), \"conf\")\n",
    "APPS_LOGS_DIR = os.path.join(os.getcwd(), \"logs\")\n",
    "LOG4J_PROP_FILE = os.path.join(APPS_CONF_DIR, \"pyspark-log4j-{}.properties\".format(NORMALIZED_APP_NAME))\n",
    "LOG_FILE = os.path.join(APPS_LOGS_DIR, 'pyspark-{}.log'.format(NORMALIZED_APP_NAME))\n",
    "EXTRA_JAVA_OPTIONS = (\n",
    "    \"-Dlog4j.configuration=file://{} \"\n",
    "    \"-Dspark.hadoop.dfs.replication=1 \"\n",
    "    \"-Dhttps.protocols=TLSv1.0,TLSv1.1,TLSv1.2,TLSv1.3\"\n",
    "    .format(LOG4J_PROP_FILE)\n",
    ")\n",
    "\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory in [APPS_CONF_DIR, APPS_LOGS_DIR, APPS_TMP_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('/opt'))\n",
    "template = env.get_template(\"pyspark_log4j.properties.template\")\n",
    "template.stream(logfile=LOG_FILE).dump(LOG4J_PROP_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(APP_NAME)\n",
    "    \n",
    "    # Master URI/configuration\n",
    "    .master(\"k8s://https://10.32.7.103:6443\")\n",
    "    \n",
    "    .config(\"spark.driver.host\", LOCAL_IP)\n",
    "    \n",
    "    # Web-UI port for your Spark-app\n",
    "    .config(\"spark.ui.port\", \"4040\")\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    \n",
    "    # How many CPU cores allocate to driver process\n",
    "    .config(\"spark.driver.cores\", \"2\")\n",
    "    \n",
    "    # How many RAM allocate to driver process\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    \n",
    "    # How many executors to create\n",
    "    .config(\"spark.executor.instances\", \"3\")\n",
    "    \n",
    "    # How many CPU cores allocate to each executor\n",
    "    .config(\"spark.executor.cores\", '2')\n",
    "    \n",
    "    # How many RAM allocate to each executor\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    \n",
    "    # How many extra RAM allocate to each executor pod to handle with JVM overheads\n",
    "    # Total pod RAM = 'spark.executor.memory' + ('spark.executor.memory' * 'spark.kubernetes.memoryOverheadFactor')\n",
    "    .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.2\")\n",
    "    \n",
    "    # How many RAM from the pool allocate to store the data\n",
    "    # Additional info: https://spark.apache.org/docs/latest/tuning.html#memory-management-overview\n",
    "    .config(\"spark.memory.fraction\", \"0.6\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\n",
    "    \n",
    "    .config(\"spark.network.timeout\", \"180s\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "    .config(\"spark.driver.extraJavaOptions\", EXTRA_JAVA_OPTIONS)\n",
    "    \n",
    "    # Namespace to create executor pods. You are allowed to create pods only in your own namespace\n",
    "    .config(\"spark.kubernetes.namespace\", LOGIN)\n",
    "    \n",
    "    # Extra labels to your driver/executor pods in Kubernetes\n",
    "    .config(\"spark.kubernetes.driver.label.appname\", APP_NAME)\n",
    "    .config(\"spark.kubernetes.executor.label.appname\", APP_NAME)\n",
    "    \n",
    "    # Spark executor image\n",
    "    .config(\"spark.kubernetes.container.image\", f\"node03.st:5000/spark-executor:{LOGIN}\")\n",
    "\n",
    "    .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "    \n",
    "    # If true - delete completed/failed pods. \n",
    "    # If your executors goes down you can set 'false' to check logs and troubleshoot your app.\n",
    "    .config(\"spark.kubernetes.executor.deleteOnTermination\", \"true\")\n",
    "    \n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"hdfs:///shared/bigdata20\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itmo_posts_df = spark.read.json(f\"{DATA_PATH}/posts_api.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ITMO posts\n",
    "\n",
    "This dataset consists of posts from the [ITMO group](https://vk.com/club94) in social network VK.\n",
    "\n",
    "Dataset structure is complex and the part of dataframe is provided below.\n",
    "\n",
    "```\n",
    "root\n",
    " |-- attachments: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    "...\n",
    " |-- comments: struct (nullable = true)\n",
    " |    |-- can_post: long (nullable = true)\n",
    " |    |-- count: long (nullable = true)\n",
    " |    |-- groups_can_post: boolean (nullable = true)\n",
    " |-- copy_history: array (nullable = true)\n",
    " |    |-- element: struct (containsNull = true)\n",
    " |    |    |-- attachments: array (nullable = true)\n",
    "...\n",
    " |    |    |-- date: long (nullable = true)\n",
    " |    |    |-- from_id: long (nullable = true)\n",
    " |    |    |-- geo: struct (nullable = true)\n",
    "...\n",
    " |    |    |-- id: long (nullable = true)\n",
    " |    |    |-- owner_id: long (nullable = true)\n",
    " |    |    |-- post_source: struct (nullable = true)\n",
    "...\n",
    " |    |    |-- post_type: string (nullable = true)\n",
    " |    |    |-- signer_id: long (nullable = true)\n",
    " |    |    |-- text: string (nullable = true)\n",
    " |-- copyright: struct (nullable = true)\n",
    "...\n",
    " |-- date: long (nullable = true)\n",
    " |-- from_id: long (nullable = true)\n",
    " |-- geo: struct (nullable = true)\n",
    "...\n",
    " |-- id: long (nullable = true)\n",
    " |-- key: string (nullable = true)\n",
    " |-- likes: struct (nullable = true)\n",
    " |    |-- can_like: long (nullable = true)\n",
    " |    |-- can_publish: long (nullable = true)\n",
    " |    |-- count: long (nullable = true)\n",
    " |    |-- user_likes: long (nullable = true)\n",
    " |-- marked_as_ads: long (nullable = true)\n",
    " |-- owner_id: long (nullable = true)\n",
    " |-- post_source: struct (nullable = true)\n",
    " |    |-- data: string (nullable = true)\n",
    " |    |-- platform: string (nullable = true)\n",
    " |    |-- type: string (nullable = true)\n",
    " |-- post_type: string (nullable = true)\n",
    " |-- reposts: struct (nullable = true)\n",
    " |    |-- count: long (nullable = true)\n",
    " |    |-- user_reposted: long (nullable = true)\n",
    " |-- signer_id: long (nullable = true)\n",
    " |-- text: string (nullable = true)\n",
    " |-- unavailable: string (nullable = true)\n",
    " |-- views: struct (nullable = true)\n",
    " |    |-- count: long (nullable = true)\n",
    "```\n",
    "\n",
    "To show the entire schema you can use dataframe's method `.printSchema()`.\n",
    "\n",
    "- Field `owner_id` indicates post owner (group or user) id. \n",
    "- Filed `id` indicates post id. **Post ids are not unique!**\n",
    "- Field `copy_history` represent the reposts chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_posts_likes_df = spark.read.parquet(f\"{DATA_PATH}/followers_posts_likes.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Followers posts likes\n",
    "\n",
    "This dataset contains information about the likes are made by ITMO group followers.\n",
    "\n",
    "The structure is quite simple:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- itemType: string (nullable = true)\n",
    " |-- ownerId: integer (nullable = true)\n",
    " |-- itemId: integer (nullable = true)\n",
    " |-- likerId: integer (nullable = true)\n",
    "```\n",
    "\n",
    "- Field `itemType` indicates the post type (original post or repost).\n",
    "- Field `ownerId` indicates the group/user that **received** the like.\n",
    "- Field `itemId` indicates object id (post/repost) that received the like.\n",
    "- Filed `likerId` indicates id of the user/group **made** the like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "followers_posts_df = spark.read.json(f\"{DATA_PATH}/followers_posts_api_final.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Followers posts\n",
    "\n",
    "The dataset contains posts of the ITMO group followers\n",
    "\n",
    "The structure is the same with `itmo_posts_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1a\n",
    "\n",
    "In this task you have to find the top of posts by its' likes count. \n",
    "\n",
    "If multiple posts have the same count - sort them by `id` in ascending order.\n",
    "\n",
    "Dataset for the task: `itmo_posts_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_1a(df: \"pyspark.sql.dataframe.DataFrame\", \n",
    "            F: \"pyspark.sql.functions\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    # Place your code to transform DataFrame here\n",
    "    modified_df = df.select(\n",
    "            F.col(\"id\").name(\"post_id\"), \n",
    "            F.col(\"likes.count\").name(\"likes_count\")\n",
    "        ).orderBy(\n",
    "            F.col(\"likes_count\").desc(),\n",
    "            F.col(\"post_id\").asc()\n",
    "    )\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|post_id|likes_count|\n",
      "+-------+-----------+\n",
      "|  32022|       1637|\n",
      "|  35068|       1629|\n",
      "|  17492|       1516|\n",
      "|  18526|       1026|\n",
      "|  19552|        955|\n",
      "|  41468|        952|\n",
      "|  19419|        868|\n",
      "|  29046|        824|\n",
      "|  32546|        786|\n",
      "|  24085|        765|\n",
      "+-------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_1a = task_1a(\n",
    "    df=itmo_posts_df,\n",
    "    F=F\n",
    ")\n",
    "result_1a.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference output:\n",
    "\n",
    "```\n",
    "+-------+-----------+\n",
    "|post_id|likes_count|\n",
    "+-------+-----------+\n",
    "|  32022|       1637|\n",
    "|  35068|       1629|\n",
    "|  17492|       1516|\n",
    "|  18526|       1026|\n",
    "|  19552|        955|\n",
    "|  41468|        952|\n",
    "|  19419|        868|\n",
    "|  29046|        824|\n",
    "|  32546|        786|\n",
    "|  24085|        765|\n",
    "+-------+-----------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1b\n",
    "\n",
    "In this task you have to find the top of posts by its' comments count. \n",
    "\n",
    "If multiple posts have the same count - sort them by `id` in ascending order.\n",
    "\n",
    "Dataset for the task: `itmo_posts_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_1b(df: \"pyspark.sql.dataframe.DataFrame\", \n",
    "            F: \"pyspark.sql.functions\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    # Place your code to transform DataFrame here\n",
    "    modified_df = df.select(\n",
    "            F.col(\"id\").name(\"post_id\"), \n",
    "            F.col(\"comments.count\").name(\"comments_count\")\n",
    "        ).orderBy(\n",
    "            F.col(\"comments_count\").desc(),\n",
    "            F.col(\"post_id\").asc()\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|post_id|comments_count|\n",
      "+-------+--------------+\n",
      "|  24085|           850|\n",
      "|  22540|           250|\n",
      "|  27722|           192|\n",
      "|   8285|           148|\n",
      "|  26860|           113|\n",
      "|  13571|           107|\n",
      "|  39294|           104|\n",
      "|  36680|            96|\n",
      "|  26006|            92|\n",
      "|  41739|            92|\n",
      "+-------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_1b = task_1b(\n",
    "    df=itmo_posts_df,\n",
    "    F=F\n",
    ")\n",
    "result_1b.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference output:\n",
    "\n",
    "```\n",
    "+-------+--------------+\n",
    "|post_id|comments_count|\n",
    "+-------+--------------+\n",
    "|  24085|           850|\n",
    "|  22540|           250|\n",
    "|  27722|           192|\n",
    "|   8285|           148|\n",
    "|  26860|           113|\n",
    "|  13571|           107|\n",
    "|  39294|           104|\n",
    "|  36680|            96|\n",
    "|  26006|            92|\n",
    "|  41739|            92|\n",
    "+-------+--------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1c\n",
    "\n",
    "In this task you have to find the top of posts by its' reposts count. \n",
    "\n",
    "If multiple posts have the same count - sort them by `id` in ascending order.\n",
    "\n",
    "Dataset for the task: `itmo_posts_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_1c(df: \"pyspark.sql.dataframe.DataFrame\", \n",
    "            F: \"pyspark.sql.functions\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    # Place your code to transform DataFrame here\n",
    "    modified_df = df.select(\n",
    "            F.col(\"id\").name(\"post_id\"), \n",
    "            F.col(\"reposts.count\").name(\"reposts_count\")\n",
    "        ).orderBy(\n",
    "            F.col(\"reposts_count\").desc(),\n",
    "            F.col(\"post_id\").asc()\n",
    "    )\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|post_id|reposts_count|\n",
      "+-------+-------------+\n",
      "|  17492|          334|\n",
      "|  19552|          246|\n",
      "|  32022|          210|\n",
      "|  11842|          129|\n",
      "|  19419|          126|\n",
      "|  13532|          110|\n",
      "|  17014|          105|\n",
      "|  35068|          101|\n",
      "|  41266|           92|\n",
      "|  12593|           90|\n",
      "+-------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_1c = task_1c(\n",
    "    df=itmo_posts_df,\n",
    "    F=F\n",
    ")\n",
    "result_1c.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference output:\n",
    "\n",
    "```\n",
    "+-------+-------------+\n",
    "|post_id|reposts_count|\n",
    "+-------+-------------+\n",
    "|  17492|          334|\n",
    "|  19552|          246|\n",
    "|  32022|          210|\n",
    "|  11842|          129|\n",
    "|  19419|          126|\n",
    "|  13532|          110|\n",
    "|  17014|          105|\n",
    "|  35068|          101|\n",
    "|  41266|           92|\n",
    "|  12593|           90|\n",
    "+-------+-------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2a\n",
    "\n",
    "In this task you have to find the top users by likes they are **received**. \n",
    "\n",
    "If multiple users have the same count - sort them by `id` in ascending order.\n",
    "\n",
    "In this task you'll have to aggregate the data.\n",
    "\n",
    "Dataset for the task: `followers_posts_likes_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_2a(df: \"pyspark.sql.dataframe.DataFrame\", \n",
    "            F: \"pyspark.sql.functions\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    # Place your code to transform DataFrame here\n",
    "    modified_df = df.groupBy(\"ownerId\")\\\n",
    "        .agg(F.count(\"ownerId\").name(\"count\"))\\\n",
    "        .orderBy(\n",
    "            F.col(\"count\").desc(),\n",
    "            F.col(\"ownerId\").asc()\n",
    "    )\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  ownerId|count|\n",
      "+---------+-----+\n",
      "|289390075|82297|\n",
      "|   327458|57697|\n",
      "|119920644|57084|\n",
      "|273486249|54882|\n",
      "| 25317378|48425|\n",
      "|150371150|44686|\n",
      "|187877260|35507|\n",
      "|   715211|30346|\n",
      "|180124822|25364|\n",
      "| 17885170|24749|\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_2a = task_2a(\n",
    "    df=followers_posts_likes_df,\n",
    "    F=F\n",
    ")\n",
    "result_2a.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference output:\n",
    "\n",
    "```\n",
    "+---------+-----+\n",
    "|  ownerId|count|\n",
    "+---------+-----+\n",
    "|289390075|82297|\n",
    "|   327458|57697|\n",
    "|119920644|57084|\n",
    "|273486249|54882|\n",
    "| 25317378|48425|\n",
    "|150371150|44686|\n",
    "|187877260|35507|\n",
    "|   715211|30346|\n",
    "|180124822|25364|\n",
    "| 17885170|24749|\n",
    "+---------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2b\n",
    "\n",
    "In this task you have to find the top users by reposts they are **made**. \n",
    "\n",
    "You can track the reposts (and check if the post if repost) using field `copy_history`.\n",
    "\n",
    "If multiple users have the same count - sort them by `id` in ascending order.\n",
    "\n",
    "In this task you'll have to aggregate the data.\n",
    "\n",
    "Dataset for the task: `followers_posts_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_2b(df: \"pyspark.sql.dataframe.DataFrame\", \n",
    "            F: \"pyspark.sql.functions\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    # Place your code to transform DaraFrame here\n",
    "    modified_df = df.where(F.col(\"copy_history\").isNotNull())\\\n",
    "                    .select(\n",
    "                        F.col(\"owner_id\"),\n",
    "                        F.col(\"copy_history.id\").name(\"src_post_id\"),\n",
    "                    ).groupBy(\"owner_id\").agg(\n",
    "                        F.count(\"src_post_id\").name(\"count\"))\\\n",
    "                        .orderBy(\n",
    "                            F.col(\"count\").desc(),\n",
    "                            F.col(\"owner_id\").asc()\n",
    "                    )\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "| owner_id|count|\n",
      "+---------+-----+\n",
      "|  2547211|37742|\n",
      "|357231922|23349|\n",
      "|168543860|18429|\n",
      "| 25646344|11122|\n",
      "|176861294| 9022|\n",
      "|524656784| 7242|\n",
      "|    29840| 7164|\n",
      "|143207077| 7161|\n",
      "|141687240| 6804|\n",
      "|459339006| 6741|\n",
      "+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_2b = task_2b(\n",
    "    df=followers_posts_df,\n",
    "    F=F\n",
    ")\n",
    "result_2b.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference output:\n",
    "\n",
    "```\n",
    "+---------+-----+\n",
    "| owner_id|count|\n",
    "+---------+-----+\n",
    "|  2547211|37742|\n",
    "|357231922|23349|\n",
    "|168543860|18429|\n",
    "| 25646344|11122|\n",
    "|176861294| 9022|\n",
    "|524656784| 7242|\n",
    "|    29840| 7164|\n",
    "|143207077| 7161|\n",
    "|141687240| 6804|\n",
    "|459339006| 6741|\n",
    "+---------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "\n",
    "In this task you have to find users' posts that are reposted **from ITMO group** and collect ids of such posts in the list.\n",
    "\n",
    "You can track the reposts using field `copy_history`. Only reposts **directly** from ITMO should be collected. Repost of the repost **shouldn't be collected**.\n",
    "\n",
    "The final result should be `ITMO post id - [list users' post ids] - reposts count`.\n",
    "\n",
    "Users' post ids in the list should be sorted in ascending order. Dataframe should be sorted by reposts count in descending order.\n",
    "\n",
    "If multiple posts have the same count - sort them by `id` in ascending order.\n",
    "\n",
    "In this task you'll have to filter and aggregate the data.\n",
    "\n",
    "Dataset for the task: `followers_posts_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_3(df: \"pyspark.sql.dataframe.DataFrame\", \n",
    "           F: \"pyspark.sql.functions\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    # Place your code to transform DaraFrame here\n",
    "    modified_df = df.where(F.col(\"copy_history\").isNotNull())\\\n",
    "                    .select(\n",
    "                        F.col(\"id\").name(\"user_post_id\"),\n",
    "                        F.col(\"copy_history.id\").getItem(0).name(\"group_post_id\"))\\\n",
    "                    .where(F.col(\"copy_history.owner_id\").getItem(0) == -94)\\\n",
    "                    .groupBy(\"group_post_id\")\\\n",
    "                    .agg(F.sort_array(F.collect_list(\"user_post_id\")).name(\"user_post_ids\")) \\\n",
    "                    .withColumn(\"reposts_count\", F.size(\"user_post_ids\"))\\\n",
    "                    .orderBy(\n",
    "                        F.col(\"reposts_count\").desc(),\n",
    "                        F.col(\"group_post_id\").asc()\n",
    "                    )\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-------------+\n",
      "|group_post_id|       user_post_ids|reposts_count|\n",
      "+-------------+--------------------+-------------+\n",
      "|        41266|[26, 53, 87, 88, ...|           30|\n",
      "|        41468|[89, 390, 400, 44...|           25|\n",
      "|        42482|[264, 713, 1190, ...|           10|\n",
      "|    456239414|[25, 1063, 1266, ...|           10|\n",
      "|        40090|[32, 349, 463, 13...|            9|\n",
      "|        38740|[185, 1060, 1133,...|            8|\n",
      "|        39259|[822, 1205, 1492,...|            8|\n",
      "|        41207|[958, 1288, 2960,...|            6|\n",
      "|        41546|[666, 939, 1161, ...|            6|\n",
      "|        41721|[8, 274, 2801, 38...|            6|\n",
      "+-------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_3 = task_3(\n",
    "    df=followers_posts_df,\n",
    "    F=F\n",
    ")\n",
    "result_3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference output:\n",
    "\n",
    "```\n",
    "+-------------+--------------------+-------------+\n",
    "|group_post_id|       user_post_ids|reposts_count|\n",
    "+-------------+--------------------+-------------+\n",
    "|        41266|[26, 53, 87, 88, ...|           30|\n",
    "|        41468|[89, 390, 400, 44...|           25|\n",
    "|        42482|[264, 713, 1190, ...|           10|\n",
    "|    456239414|[25, 1063, 1266, ...|           10|\n",
    "|        40090|[32, 349, 463, 13...|            9|\n",
    "|        38740|[185, 1060, 1133,...|            8|\n",
    "|        39259|[822, 1205, 1492,...|            8|\n",
    "|        41207|[958, 1288, 2960,...|            6|\n",
    "|        41546|[666, 939, 1161, ...|            6|\n",
    "|        41721|[8, 274, 2801, 38...|            6|\n",
    "+-------------+--------------------+-------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "\n",
    "In this task you have to find emojis in followers posts. Then, you'll have to find top positive emojis, top neutral emojis, top negative emojis.\n",
    "\n",
    "These tops should be returned in separate dataframes.\n",
    "\n",
    "For this task you are allowed to use `emoji==0.6.0` package (already installed in your images).\n",
    "This package import should be done inside the function.\n",
    "\n",
    "To recognize positive, negative and neutral emojis you can use provided `emojis_sentiment_dict` \n",
    "dictionary with the structure:\n",
    "\n",
    "```\n",
    "{\n",
    " 'ðŸ˜‚': 'positive',\n",
    " 'â¤': 'positive',\n",
    " 'â™¥': 'positive',\n",
    " 'ðŸ˜': 'positive',\n",
    " 'ðŸ˜­': 'negative',\n",
    " 'ðŸ˜˜': 'positive',\n",
    " 'ðŸ’—': 'positive',\n",
    " 'â˜…': 'neutral',\n",
    " 'â–ˆ': 'neutral',\n",
    " 'â˜€': 'positive',\n",
    " ...\n",
    "}\n",
    "```\n",
    "\n",
    "This dictionary is obtained from the [research](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0144296)\n",
    "and the [shared results](https://www.clarin.si/repository/xmlui/handle/11356/1048).\n",
    "\n",
    "Sort the each top by emojis count in descending order. If count for multiple emojis the same, sort them in alphabetical order.\n",
    "\n",
    "In this task you'll have to create your own [UDF (User Defined Function)](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.functions.udf.html).\n",
    "Also, you'll have to [broadcast](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.Broadcast.html)\n",
    "`emojis_sentiment_dict` to share the dictionary with your executors and enrich your dataframe with sentiment.\n",
    "\n",
    "`broadcast_func` in task function signature is the method `.broadcast()` from `spark.sparkContext`.\n",
    "\n",
    "Dataset for the task: `followers_posts_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/jovyan/shared-data/bigdata20/emojis_sentiment.json\", \"r\") as f:\n",
    "    emojis_sentiment_dict = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_4(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           T: \"pyspark.sql.types\",\n",
    "           emojis_data: dict,\n",
    "           broadcast_func: \"spark.sparkContext.broadcast\") -> 'Tuple[\"pyspark.sql.dataframe.DataFrame\"]':\n",
    "    # You are able to modify any code inside this function.\n",
    "    # Only 'emoji' package import is allowed for this task.\n",
    "    \n",
    "    import emoji\n",
    "    reg_exp = emoji.get_emoji_regexp()\n",
    "\n",
    "    # 0) Helpers\n",
    "    @F.udf(returnType=T.ArrayType(T.StringType()))\n",
    "    def getEmojiSlice(text):\n",
    "        emojis = []\n",
    "        for reg_match in reg_exp.finditer(text):\n",
    "            emojis.append(reg_match.group())\n",
    "        return emojis\n",
    "\n",
    "    \n",
    "    broarcasted_dict = broadcast_func(emojis_data)\n",
    "    @F.udf(returnType=T.StringType())\n",
    "    def getSentiment(col):\n",
    "        return broarcasted_dict.value.get(col, None)\n",
    "\n",
    "    \n",
    "    # 1) Gather emoji's from df\n",
    "    raw_text_emoji_df = df.where(F.col(\"text\").isNotNull())\\\n",
    "                            .select(\n",
    "                                F.col(\"id\"), \n",
    "                                F.col(\"text\")\n",
    "                            )\\\n",
    "                            .withColumn(\"emojis\", getEmojiSlice(\"text\"))\\\n",
    "                            .where(F.size(\"emojis\") > 0)\\\n",
    "                            .select(F.col(\"id\").name(\"postID\"), F.col(\"emojis\"))\n",
    "\n",
    "    # 2) Get all emojis counted\n",
    "    emoji_df = raw_text_emoji_df.select(F.explode(\"emojis\").name(\"emoji\"))\\\n",
    "                               .groupBy(\"emoji\")\\\n",
    "                               .agg(F.count(\"emoji\").name(\"count\"))\\\n",
    "                                .withColumn(\"sentiment\", getSentiment(F.col(\"emoji\")))\n",
    "\n",
    "    positive_emojis = emoji_df.where(F.col(\"sentiment\") == \"positive\")\\\n",
    "                                .select(\n",
    "                                    F.col(\"emoji\"),\n",
    "                                    F.col(\"count\"))\\\n",
    "                                .orderBy(F.col(\"count\").desc())\n",
    "    \n",
    "    neutral_emojis = emoji_df.where(F.col(\"sentiment\") == \"neutral\")\\\n",
    "                                .select(\n",
    "                                    F.col(\"emoji\"),\n",
    "                                    F.col(\"count\"))\\\n",
    "                                .orderBy(F.col(\"count\").desc())\n",
    "    \n",
    "    negative_emojis = emoji_df.where(F.col(\"sentiment\") == \"negative\")\\\n",
    "                                .select(\n",
    "                                    F.col(\"emoji\"),\n",
    "                                    F.col(\"count\"))\\\n",
    "                                .orderBy(F.col(\"count\").desc())\n",
    "    \n",
    "    return (positive_emojis, neutral_emojis, negative_emojis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_emojis, neutral_emojis, negative_emojis = task_4(\n",
    "    df=followers_posts_df,\n",
    "    F=F,\n",
    "    T=T,\n",
    "    emojis_data=emojis_sentiment_dict,\n",
    "    broadcast_func=spark.sparkContext.broadcast\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|emoji|count|\n",
      "+-----+-----+\n",
      "|    â¤|15238|\n",
      "|   ðŸ‘| 6538|\n",
      "|   ðŸ˜‚| 5554|\n",
      "|   ðŸŒº| 5470|\n",
      "|   ðŸ˜| 4583|\n",
      "|   ðŸ”»| 4115|\n",
      "|   ðŸŽ€| 3788|\n",
      "|   ðŸ˜‰| 3687|\n",
      "|    â˜€| 3243|\n",
      "|    âœ…| 2985|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+-----+\n",
      "|emoji|count|\n",
      "+-----+-----+\n",
      "|   ðŸ”¥|11774|\n",
      "|    Â®| 9144|\n",
      "|   ðŸ’¥| 4669|\n",
      "|    â—| 3563|\n",
      "|    âœ¨| 2833|\n",
      "|   ðŸ’°| 1365|\n",
      "|    Â©| 1150|\n",
      "|   ðŸ’£| 1124|\n",
      "|    âœ”| 1086|\n",
      "|    âš | 1040|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+-----+\n",
      "|emoji|count|\n",
      "+-----+-----+\n",
      "|   ðŸ“Œ| 1427|\n",
      "|   ðŸ˜­|  483|\n",
      "|   ðŸ˜”|  251|\n",
      "|   ðŸ’”|  251|\n",
      "|    âž–|  249|\n",
      "|   ðŸ˜¨|  241|\n",
      "|   ðŸ˜¤|  206|\n",
      "|   ðŸ”«|  194|\n",
      "|   ðŸ˜²|  192|\n",
      "|   ðŸ˜¾|  165|\n",
      "+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "positive_emojis.show(10)\n",
    "neutral_emojis.show(10)\n",
    "negative_emojis.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference output:\n",
    "\n",
    "```\n",
    "+-----+-----+\n",
    "|emoji|count|\n",
    "+-----+-----+\n",
    "|    â¤|15238|\n",
    "|   ðŸ‘| 6538|\n",
    "|   ðŸ˜‚| 5554|\n",
    "|   ðŸŒº| 5470|\n",
    "|   ðŸ˜| 4583|\n",
    "|   ðŸ”»| 4115|\n",
    "|   ðŸŽ€| 3788|\n",
    "|   ðŸ˜‰| 3687|\n",
    "|    â˜€| 3243|\n",
    "|    âœ…| 2985|\n",
    "+-----+-----+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "+-----+-----+\n",
    "|emoji|count|\n",
    "+-----+-----+\n",
    "|   ðŸ”¥|11774|\n",
    "|    Â®| 9144|\n",
    "|   ðŸ’¥| 4669|\n",
    "|    â—| 3563|\n",
    "|    âœ¨| 2833|\n",
    "|   ðŸ’°| 1365|\n",
    "|    Â©| 1150|\n",
    "|   ðŸ’£| 1124|\n",
    "|    âœ”| 1086|\n",
    "|    âš | 1040|\n",
    "+-----+-----+\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```\n",
    "+-----+-----+\n",
    "|emoji|count|\n",
    "+-----+-----+\n",
    "|   ðŸ“Œ| 1427|\n",
    "|   ðŸ˜­|  483|\n",
    "|   ðŸ’”|  251|\n",
    "|   ðŸ˜”|  251|\n",
    "|    âž–|  249|\n",
    "|   ðŸ˜¨|  241|\n",
    "|   ðŸ˜¤|  206|\n",
    "|   ðŸ”«|  194|\n",
    "|   ðŸ˜²|  192|\n",
    "|   ðŸ˜¾|  165|\n",
    "+-----+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5\n",
    "\n",
    "In this task you have to find users' probable fans.\n",
    "\n",
    "User A fans are users B from whom A received the most likes.\n",
    "\n",
    "You'll have to find top-N fans for each user A. \n",
    "\n",
    "The result should be ordered by user A `id` and received likes count from user B. If multiple users B gave the same likes count, order them by their `id`.\n",
    "\n",
    "In this task you'll have to use [window-function](https://sparkbyexamples.com/pyspark/pyspark-window-functions/).\n",
    "\n",
    "Dataset for the task: `followers_posts_likes_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_5(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           W: \"pyspark.sql.window.Window\",\n",
    "           top_n_likers: int) -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    # Place your code to transform DaraFrame here\n",
    "    modified_df = df.groupBy(\n",
    "                         F.col(\"ownerId\"),\n",
    "                         F.col(\"likerId\"))\\\n",
    "                    .agg(F.count(\"ownerId\").name(\"count\"))\n",
    "    \n",
    "    # removing self likes\n",
    "    modified_df = modified_df.where(modified_df.ownerId != modified_df.likerId)\n",
    "    \n",
    "    # using window function\n",
    "    windowSpec  = W.partitionBy(\"ownerId\")\\\n",
    "                    .orderBy(\n",
    "                        F.col(\"ownerId\").asc(), \n",
    "                        F.col(\"count\").desc(), \n",
    "                        F.col(\"likerId\").asc()\n",
    "    )\n",
    "    \n",
    "    modified_df = modified_df.withColumn(\"row_number\", F.row_number().over(windowSpec))\n",
    "    \n",
    "    modified_df = modified_df.where(modified_df.row_number <= top_n_likers)\\\n",
    "                             .drop(\"row_number\")\\\n",
    "                             .orderBy(\n",
    "                                F.col(\"ownerId\").asc(),\n",
    "                                F.col(\"count\").desc(),\n",
    "                                F.col(\"likerId\").asc()\n",
    "                             )\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-----+\n",
      "|ownerId|  likerId|count|\n",
      "+-------+---------+-----+\n",
      "|    637|   422527|   24|\n",
      "|    637| 13237321|   21|\n",
      "|    637|   162706|   20|\n",
      "|    637|   392577|   16|\n",
      "|    637|151081369|   16|\n",
      "|    637| 49547307|   15|\n",
      "|    637|145422426|   15|\n",
      "|    637|   407844|   14|\n",
      "|    637|    94399|   13|\n",
      "|    637|   359267|   13|\n",
      "|   1087| 84798348|   48|\n",
      "|   1087|354351777|   30|\n",
      "|   1087|230753056|   26|\n",
      "|   1087|255139140|   23|\n",
      "|   1087|499354771|   22|\n",
      "+-------+---------+-----+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_5 = task_5(\n",
    "    df=followers_posts_likes_df,\n",
    "    F=F,\n",
    "    W=Window,\n",
    "    top_n_likers=10\n",
    ")\n",
    "result_5.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference output:\n",
    "\n",
    "```\n",
    "+-------+---------+-----+\n",
    "|ownerId|  likerId|count|\n",
    "+-------+---------+-----+\n",
    "|    637|   422527|   24|\n",
    "|    637| 13237321|   21|\n",
    "|    637|   162706|   20|\n",
    "|    637|   392577|   16|\n",
    "|    637|151081369|   16|\n",
    "|    637| 49547307|   15|\n",
    "|    637|145422426|   15|\n",
    "|    637|   407844|   14|\n",
    "|    637|    94399|   13|\n",
    "|    637|   359267|   13|\n",
    "|   1087| 84798348|   48|\n",
    "|   1087|354351777|   30|\n",
    "|   1087|230753056|   26|\n",
    "|   1087|255139140|   23|\n",
    "|   1087|499354771|   22|\n",
    "+-------+---------+-----+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "\n",
    "This task is similar to Task 5. You have to find probable friends.\n",
    "\n",
    "User A and user B are friends if they gave each other maximum number of likes. I.e. user A is the top \"fan\" for user B and user B is the top \"fan\" for user A.\n",
    "\n",
    "If multiple users B gave the same maximum number of likes to user A, they all may be friends for user A.\n",
    "\n",
    "User A fans are users B from whom A received the most likes.\n",
    "\n",
    "The result should be the dataframe `user_a - user_b - likes_from_a - likes_from_b - mutual_likes`. Mutual likes is just a sum of likes from A and likes from B.\n",
    "\n",
    "The result shouldn't contain identical friend pairs (i.e. `user_a = 123, user_b = 456 and user_a = 456, user_b = 123` are identical pairs).\n",
    "\n",
    "The result should be ordered by mutual likes in descending order. If the mutual likes count is the same for different pairs, then sort them by user A `id` and user B `id` in ascending order.\n",
    "\n",
    "In this task you'll have to use joins.\n",
    "\n",
    "Dataset for the task: `followers_posts_likes_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_6(df: \"pyspark.sql.dataframe.DataFrame\",\n",
    "           F: \"pyspark.sql.functions\",\n",
    "           W: \"pyspark.sql.window.Window\") -> \"pyspark.sql.dataframe.DataFrame\":\n",
    "    # Place your code to transform DaraFrame here    \n",
    "    userA = df.groupBy(\n",
    "                    F.col(\"likerId\"),\n",
    "                    F.col(\"ownerId\"))\\\n",
    "                .agg(F.count(\"ownerId\").name(\"count\"))\n",
    "\n",
    "    # Removing self likes\n",
    "    userA = userA.where(userA.ownerId != userA.likerId)\n",
    "\n",
    "    # SEARCH LIKES\n",
    "    window = W.partitionBy(\"likerId\").orderBy(\n",
    "            # F.col(\"ownerId\").asc(),\n",
    "            F.col(\"count\").desc(),\n",
    "            F.col(\"likerId\").asc()\n",
    "    )\n",
    "\n",
    "\n",
    "    userA = userA.withColumn(\"max_likes\", F.max(\"count\").over(window))\\\n",
    "                 .where(F.col(\"count\") == F.col(\"max_likes\"))\\\n",
    "                 .drop(\"max_likes\") \n",
    "    # Left only top likers (with \"count\" == topG count within \"ownerId\")\n",
    "    \n",
    "    userB = userA.alias(\"userB\")\n",
    "    \n",
    "    modified_df = userA.alias(\"userA\").join(\n",
    "                                            userB, \\\n",
    "                                            (F.col(\"userA.ownerId\") == F.col(\"userB.likerId\")) &\n",
    "                                            (F.col(\"userA.likerId\") == F.col(\"userB.ownerId\")) &\n",
    "                                            (F.col(\"userA.ownerId\") < F.col(\"userB.ownerId\")),\n",
    "                                            \"inner\")\\\n",
    "                                        .select(\n",
    "                                            F.col(\"userA.ownerId\").name(\"user_a\"),\n",
    "                                            F.col(\"userB.ownerId\").name(\"user_b\"),\n",
    "                                            F.col(\"userB.count\").name(\"likes_from_a\"),\n",
    "                                            F.col(\"userA.count\").name(\"likes_from_b\")\n",
    "                                        )\n",
    "\n",
    "    modified_df = modified_df.withColumn(\"mutual_likes\", modified_df.likes_from_b + modified_df.likes_from_a)\n",
    "\n",
    "    # Removing identical (user_a = 123, user_b = 456 and user_a = 456, user_b = 123) entries\n",
    "    modified_df = modified_df.withColumn(\n",
    "        \"sorted_ids\",\n",
    "        F.concat(\n",
    "            F.least(F.col(\"user_a\"), F.col(\"user_b\")),\n",
    "            F.lit(\"_\"),\n",
    "            F.greatest(F.col(\"user_a\"), F.col(\"user_b\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    modified_df = modified_df.dropDuplicates([\"sorted_ids\"])\n",
    "\n",
    "    modified_df = modified_df.select(\n",
    "                                F.col(\"user_a\"), \n",
    "                                F.col(\"user_b\"), \n",
    "                                F.col(\"likes_from_a\"), \n",
    "                                F.col(\"likes_from_b\"), \n",
    "                                F.col(\"mutual_likes\")\n",
    "    )\n",
    "\n",
    "    modified_df = modified_df.sort(F.col(\"mutual_likes\").desc(), F.col(\"user_a\").asc(), F.col(\"user_b\").asc())\n",
    "    \n",
    "    return modified_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_6 = task_6(\n",
    "    df=followers_posts_likes_df,\n",
    "    F=F,\n",
    "    W=Window\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+------------+------------+\n",
      "|   user_a|   user_b|likes_from_a|likes_from_b|mutual_likes|\n",
      "+---------+---------+------------+------------+------------+\n",
      "| 13675440|183535934|         161|         100|         261|\n",
      "|207134315|208946862|          31|          52|          83|\n",
      "|145105762|267301242|          52|          28|          80|\n",
      "|155963006|162366815|          12|          57|          69|\n",
      "|   135451| 18737802|          45|          18|          63|\n",
      "| 53368685|322831238|          11|          52|          63|\n",
      "|121608397|441534917|           3|          57|          60|\n",
      "|101767883|188548515|          52|           6|          58|\n",
      "|209077977|272076217|          40|          18|          58|\n",
      "|   460957|   715211|          53|           2|          55|\n",
      "|    45781|    58440|           4|          47|          51|\n",
      "| 19261491|229861638|          23|          26|          49|\n",
      "| 52612744| 53720099|          32|          17|          49|\n",
      "|   667303|  1113545|           4|          44|          48|\n",
      "| 66022003| 95356919|          23|          22|          45|\n",
      "+---------+---------+------------+------------+------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_6.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference output:\n",
    "```\n",
    "+---------+---------+------------+------------+------------+\n",
    "|   user_a|   user_b|likes_from_a|likes_from_b|mutual_likes|\n",
    "+---------+---------+------------+------------+------------+\n",
    "| 13675440|183535934|         161|         100|         261|\n",
    "|207134315|208946862|          31|          52|          83|\n",
    "|145105762|267301242|          52|          28|          80|\n",
    "|155963006|162366815|          12|          57|          69|\n",
    "|   135451| 18737802|          45|          18|          63|\n",
    "| 53368685|322831238|          11|          52|          63|\n",
    "|121608397|441534917|           3|          57|          60|\n",
    "|101767883|188548515|          52|           6|          58|\n",
    "|209077977|272076217|          40|          18|          58|\n",
    "|   460957|   715211|          53|           2|          55|\n",
    "|    45781|    58440|           4|          47|          51|\n",
    "| 19261491|229861638|          23|          26|          49|\n",
    "| 52612744| 53720099|          32|          17|          49|\n",
    "|   667303|  1113545|           4|          44|          48|\n",
    "| 66022003| 95356919|          23|          22|          45|\n",
    "+---------+---------+------------+------------+------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAQ\n",
    "\n",
    "---\n",
    "\n",
    "- Q: Spark reading file infinitely. Why?\n",
    "- A: Probably your executor haven't started yet or they cannot start. Check if your executors are running (`kubectl get pods`). If executors are running - check the logs.\n",
    "\n",
    "---\n",
    "\n",
    "- Q: I cannot create Spark app. It raise exception `Exception: Java gateway process exited before sending its port number`.\n",
    "- A: Check the path to your notebook. It doesn't have to contain space character (\" \"). Rename your directories and notebook.\n",
    "\n",
    "---\n",
    "\n",
    "- Q: My executors goes down with `OOMKilled`. How to fix it?\n",
    "- A: `OOMKilled` pod state means the pod is exceeded RAM limit. You can increase available executor RAM using config `spark.executor.memory`. Another reason - you do something wrong with your app and collect all of the data into one executor.\n",
    "\n",
    "---\n",
    "\n",
    "- Q: I got less executors than have been specified in config.\n",
    "- A: You have exceeded resources limit for your namespace. Adjust your resource consumption.\n",
    "\n",
    "---\n",
    "\n",
    "- Q: My executor failing with `Error` state.\n",
    "- A: Check the logs.\n",
    "\n",
    "---\n",
    "\n",
    "- Q: I cannot save my notebook/data and get `No space left on device` or `Quota exceeded` error from the Jupyter Server.\n",
    "- A: You have exceeded NFS-server storage quota. Delete unnecessary files from you nfs home directory (for example, logs, old data, etc)\n",
    "\n",
    "---\n",
    "\n",
    "If your question is not listed above, please contact the course staff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
